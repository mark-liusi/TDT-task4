# ç»ˆæè§£å†³æ–¹æ¡ˆï¼šåœ¨ä½ çš„æ•°æ®ä¸Šå¾®è°ƒMNISTæ¨¡å‹
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torchvision import datasets, transforms
from torch.utils.data import DataLoader, random_split
import matplotlib.pyplot as plt
import matplotlib
matplotlib.rcParams['font.sans-serif']= ['Noto Sans CJK SC','Noto Sans CJK JP','WenQuanYi Micro Hei','SimHei']  # æ”¯æŒä¸­æ–‡çš„å¤‡é€‰å­—ä½“
matplotlib.rcParams['axes.unicode_minus']= False  # å…è®¸åæ ‡è½´æ˜¾ç¤ºè´Ÿå·


# è®¾ç½®è®¾å¤‡
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ä½¿ç”¨è®¾å¤‡: {device}")

# æ•°æ®é¢„å¤„ç†
transform = transforms.Compose(
    [
        transforms.Grayscale(num_output_channels=1),
        transforms.Resize((28, 28)),
        transforms.ToTensor(),
        transforms.Normalize((0.5,), (0.5,)),  # ä½¿ç”¨æ•ˆæœè¾ƒå¥½çš„é€šç”¨æ ‡å‡†åŒ–
    ]
)

# åŠ è½½ä½ çš„æ•°æ®é›†
full_dataset = datasets.ImageFolder(root="./number", transform=transform)
print(f"æ€»æ•°æ®é‡: {len(full_dataset)}")
print(f"ç±»åˆ«: {full_dataset.classes}")

# åˆ†å‰²æ•°æ®é›†ï¼š80%è®­ç»ƒï¼Œ20%æµ‹è¯•
train_size = int(0.8 * len(full_dataset))
test_size = len(full_dataset) - train_size
train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])

print(f"è®­ç»ƒé›†å¤§å°: {len(train_dataset)}")
print(f"æµ‹è¯•é›†å¤§å°: {len(test_dataset)}")

# åˆ›å»ºæ•°æ®åŠ è½½å™¨
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)


# æ¨¡å‹å®šä¹‰ï¼ˆä¸åŸæ¨¡å‹å®Œå…¨ä¸€è‡´ï¼‰
class ImprovedNet(nn.Module):
    def __init__(self):
        super(ImprovedNet, self).__init__()
        self.fc1 = nn.Linear(28 * 28, 512)
        self.bn1 = nn.BatchNorm1d(512)
        self.fc2 = nn.Linear(512, 256)
        self.bn2 = nn.BatchNorm1d(256)
        self.fc3 = nn.Linear(256, 128)
        self.bn3 = nn.BatchNorm1d(128)
        self.fc4 = nn.Linear(128, 10)
        self.dropout = nn.Dropout(0.2)

    def forward(self, x):
        x = x.view(-1, 28 * 28)
        x = F.relu(self.bn1(self.fc1(x)))
        x = self.dropout(x)
        x = F.relu(self.bn2(self.fc2(x)))
        x = self.dropout(x)
        x = F.relu(self.bn3(self.fc3(x)))
        x = self.dropout(x)
        x = self.fc4(x)
        return x


# æ ‡ç­¾é‡æ˜ å°„å‡½æ•°
def remap_labels(labels):
    return torch.tensor([l + 1 for l in labels])


# åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
model = ImprovedNet().to(device)
model.load_state_dict(torch.load("mnist_fc_improved.pth", map_location=device))
print("âœ… æˆåŠŸåŠ è½½MNISTé¢„è®­ç»ƒæ¨¡å‹")

# å¾®è°ƒè®¾ç½®
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)  # è¾ƒå°çš„å­¦ä¹ ç‡ç”¨äºå¾®è°ƒ
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)


# è®­ç»ƒå‡½æ•°
def train_epoch(model, train_loader, criterion, optimizer, device):
    model.train()
    total_loss = 0
    correct = 0
    total = 0

    for batch_idx, (data, target) in enumerate(train_loader):
        data = data.to(device)
        target = remap_labels(target).to(device)

        optimizer.zero_grad()
        output = model(data)
        loss = criterion(output, target)
        loss.backward()
        optimizer.step()

        total_loss += loss.item()
        pred = output.argmax(dim=1, keepdim=True)
        correct += pred.eq(target.view_as(pred)).sum().item()
        total += target.size(0)

    avg_loss = total_loss / len(train_loader)
    accuracy = 100.0 * correct / total
    return avg_loss, accuracy


# æµ‹è¯•å‡½æ•°
def test_epoch(model, test_loader, criterion, device):
    model.eval()
    test_loss = 0
    correct = 0
    total = 0

    with torch.no_grad():
        for data, target in test_loader:
            data = data.to(device)
            target = remap_labels(target).to(device)

            output = model(data)
            test_loss += criterion(output, target).item()
            pred = output.argmax(dim=1, keepdim=True)
            correct += pred.eq(target.view_as(pred)).sum().item()
            total += target.size(0)

    avg_loss = test_loss / len(test_loader)
    accuracy = 100.0 * correct / total
    return avg_loss, accuracy


# å¾®è°ƒè®­ç»ƒ
print("\nå¼€å§‹å¾®è°ƒè®­ç»ƒ...")
num_epochs = 20  # å¾®è°ƒä¸éœ€è¦å¤ªå¤šè½®
train_losses = []
train_accuracies = []
test_losses = []
test_accuracies = []

for epoch in range(num_epochs):
    # è®­ç»ƒ
    train_loss, train_acc = train_epoch(
        model, train_loader, criterion, optimizer, device
    )
    train_losses.append(train_loss)
    train_accuracies.append(train_acc)

    # æµ‹è¯•
    test_loss, test_acc = test_epoch(model, test_loader, criterion, device)
    test_losses.append(test_loss)
    test_accuracies.append(test_acc)

    # å­¦ä¹ ç‡è°ƒåº¦
    scheduler.step()

    print(
        f"Epoch {epoch+1:2d}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:6.2f}%, "
        f"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:6.2f}%"
    )

# ä¿å­˜å¾®è°ƒåçš„æ¨¡å‹
torch.save(model.state_dict(), "mnist_finetuned_on_custom.pth")
print(f"\nâœ… å¾®è°ƒå®Œæˆï¼æ¨¡å‹å·²ä¿å­˜ä¸º 'mnist_finetuned_on_custom.pth'")
print(f"ğŸ‰ æœ€ç»ˆæµ‹è¯•å‡†ç¡®ç‡: {test_accuracies[-1]:.2f}%")

# ç»˜åˆ¶è®­ç»ƒæ›²çº¿
plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.plot(train_losses, label="Train Loss")
plt.plot(test_losses, label="Test Loss")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.title("Training and Test Loss")
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(train_accuracies, label="Train Accuracy")
plt.plot(test_accuracies, label="Test Accuracy")
plt.xlabel("Epoch")
plt.ylabel("Accuracy (%)")
plt.title("Training and Test Accuracy")
plt.legend()

plt.tight_layout()
plt.savefig("finetuning_curves.png", dpi=150, bbox_inches="tight")
plt.show()

print(f"\nğŸ“Š è®­ç»ƒæ›²çº¿å·²ä¿å­˜ä¸º 'finetuning_curves.png'")
print(f"ğŸ’¡ ç°åœ¨ä½ å¯ä»¥ç”¨å¾®è°ƒåçš„æ¨¡å‹è·å¾—æ›´å¥½çš„è¯†åˆ«æ•ˆæœï¼")
